{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a10b476b-2545-49f6-a022-01e596a0cb38",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stimulus playground\n",
    "This script is a playground to inspect and manipulate stimuli for a binocular rivalry (BR) experiment<br>\n",
    "In this experiment (as at April th) I aim to present objects that are associated with a color (e.g., banana - yellow, strawberry - red) in rivarous situations.<br>\n",
    "1. Show the same stimulus either in the \"correct\" and in an \"incorrect\" color (a banana in yellow and blue)\n",
    "2. Show two different stimuli that are associated with different colors but in the same color (a banana and a strawberry both in yellow)\n",
    "\n",
    "Compare the onset and sustained dominance with rivaling gratings that follow a series of non-rivaling gratings that appear to be rotating (Denison et al., 2011, Attarha et al., 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabc02fc-718e-4157-8d1f-061c8c9abb16",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stimuli\n",
    "In the following I will look at the stimuli used by Teichmann et al., 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0f4fc70-b12f-4ea2-a00a-2a39b416d67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "# use the os\n",
    "import os\n",
    "import glob\n",
    "# math and data structure\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# image processing\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17581e11-225f-4d53-a5b5-cc8468dea3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stimuli from Teichmann et al., 2020\n",
    "# provided by her OSF site (https://osf.io/tcqjh/)\n",
    "stim_dir = os.path.join('..','stimuli')\n",
    "original_stim_dir = os.path.join(stim_dir,'original')\n",
    "grey_stim_dir = os.path.join(stim_dir,'grey') # not used yet\n",
    "# get a list of all stimuli in the directory\n",
    "stimuli_full_path = glob.glob(os.path.join(original_stim_dir,'*.png'))\n",
    "stimuli_full_path.sort()\n",
    "\n",
    "# extract the image name from the stimulus path\n",
    "stimuli = [os.path.basename(stim) for stim in stimuli_full_path]\n",
    "\n",
    "# taken from original - added alpha channel\n",
    "true_color_stim_dir = os.path.join(stim_dir,'true_color')\n",
    "# increased saturation for stimuli to prevent color fusion\n",
    "max_saturation_stim_dir = os.path.join(stim_dir,'max_saturation')\n",
    "if not os.path.exists(max_saturation_stim_dir):\n",
    "    os.mkdir(max_saturation_stim_dir)\n",
    "# stimuli in HLS space inverted\n",
    "inverted_hue_stim_dir = os.path.join(stim_dir,'inverted_hue')\n",
    "if not os.path.exists(inverted_hue_stim_dir):\n",
    "    os.mkdir(inverted_hue_stim_dir)\n",
    "# stimuli in LAB space inverted\n",
    "inverted_lab_stim_dir = os.path.join(stim_dir,'inverted_lab')\n",
    "if not os.path.exists(inverted_lab_stim_dir):\n",
    "    os.mkdir(inverted_lab_stim_dir)\n",
    "# stimuli in LUV space inverted\n",
    "inverted_luv_stim_dir = os.path.join(stim_dir,'inverted_luv')\n",
    "if not os.path.exists(inverted_luv_stim_dir):\n",
    "    os.mkdir(inverted_luv_stim_dir)\n",
    "# masks\n",
    "mask_dir = os.path.join(stim_dir,'masks')\n",
    "if not os.path.exists(mask_dir):\n",
    "    os.mkdir(mask_dir)\n",
    "shine_dir = os.path.join(stim_dir,'shine_toolbox')\n",
    "compressed_true_color = os.path.join(stim_dir, 'compressed_true_color')\n",
    "if not os.path.exists(compressed_true_color):\n",
    "    os.mkdir(compressed_true_color)\n",
    "compressed_false_color = os.path.join(stim_dir, 'compressed_false_color')\n",
    "if not os.path.exists(compressed_false_color):\n",
    "    os.mkdir(compressed_false_color)\n",
    "    \n",
    "    \n",
    "# where results of image analyses are saved\n",
    "results_dir = os.path.join(stim_dir,'analysis')\n",
    "if not os.path.exists(results_dir):\n",
    "    os.mkdir(results_dir)\n",
    "ab_dist_plot_dir = os.path.join(results_dir, 'ab_distance')\n",
    "if not os.path.exists(ab_dist_plot_dir):\n",
    "    os.mkdir(ab_dist_plot_dir)\n",
    "uv_dist_plot_dir = os.path.join(results_dir, 'uv_distance')\n",
    "if not os.path.exists(uv_dist_plot_dir):\n",
    "    os.mkdir(uv_dist_plot_dir)\n",
    "hue_dist_plot_dir = os.path.join(results_dir, 'hue_distributions')\n",
    "if not os.path.exists(hue_dist_plot_dir):\n",
    "    os.mkdir(hue_dist_plot_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3004866c-20e4-48c5-9c7c-538d1eb23921",
   "metadata": {},
   "source": [
    "### Note\n",
    "Some of the stimuli taken from Teichmann were prepared with the following cell, but created \"wholes\" in the stimuli, because these had pure white pixels in the foreground.<br>\n",
    "The \"wholes\" within the images were filled using GIMP and replaced the originals.<br>\n",
    "The true originals that were replaced are now located in `original_exchanged`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "75f6fc7c-75b2-448b-b586-9a46e0a71d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read out the original stimuli and remove the background\n",
    "for stim_path, stim in zip(stimuli_full_path, stimuli):\n",
    "    # output directories\n",
    "    true_color_output_dir = os.path.join(true_color_stim_dir, stim)\n",
    "    \n",
    "    # read in the original image\n",
    "    img = cv2.imread(stim_path)\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Threshold the image to isolate the non-white pixels\n",
    "    _, thresh = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # Find the contours in the thresholded image\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Find the contour with the largest area\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    # Create a binary mask for the largest contour\n",
    "    mask = np.zeros(gray.shape, dtype=np.uint8)\n",
    "    cv2.drawContours(mask, [largest_contour], -1, (255, 255, 255), cv2.FILLED)\n",
    "\n",
    "    # Create a binary mask with the pixels inside the contour set to white\n",
    "    # and the pixels outside the contour set to black\n",
    "    binary_mask = cv2.inRange(mask, 255, 255)\n",
    "    \n",
    "    # convert image from BGR to HLS space\n",
    "    hls_img = cv2.cvtColor(img, cv2.COLOR_BGR2HLS)\n",
    "    \n",
    "    # The contour also captures empty space surrounded by object boundaries\n",
    "    # We therefore add a mask that includes all pixels with a luminance value of 255 -> white\n",
    "    white_mask = hls_img[:,:,1]==255\n",
    "    big_mask = np.logical_or(white_mask, ~binary_mask)\n",
    "    \n",
    "    # add alpha channel to original image by converting from BGR to BGRA space\n",
    "    bgra_img = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA)\n",
    "    \n",
    "    # change alpha channel of all pixels that are white to completely transparent\n",
    "    # CAUTION this results in pixels that are part of the stimulus to be transparent. \n",
    "    bgra_img[big_mask == 1] = np.array([255,255,255,0],dtype='uint8') \n",
    "    cv2.imwrite(true_color_output_dir,bgra_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d5a728-2bd2-49f8-b43b-a7485dd42c87",
   "metadata": {},
   "source": [
    "## Invert the color in the stimuli\n",
    "This is not performed in the same step because some minor adjustments were made in the true color stimuli using GIMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "5a9f9bbb-e37d-4fc8-b631-d574616bea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dist(dim1, dim2, mask):\n",
    "    # Extract the values from dim1 and dim2 where mask is 0\n",
    "    dim1_flat = dim1[~mask]\n",
    "    dim2_flat = dim2[~mask]\n",
    "    # Stack the extracted values column-wise to form a 2D array\n",
    "    true_dims = np.column_stack((dim1_flat,dim2_flat)).astype('uint8')\n",
    "\n",
    "    # Invert dim1 and dim2 using 255 - dim\n",
    "    dim1_inv = 255 - dim1\n",
    "    dim2_inv = 255 - dim2\n",
    "    # Extract the inverted values from dim1_inv and dim2_inv where mask is 0\n",
    "    dim1_inv_flat = dim1_inv[~mask]\n",
    "    dim2_inv_flat = dim2_inv[~mask]\n",
    "    # Stack the extracted inverted values column-wise to form a 2D array\n",
    "    inv_dims = np.column_stack((dim1_inv_flat, dim2_inv_flat)).astype(int)\n",
    "\n",
    "    # Compute the Euclidean distance between each pair of points in true_dims and inv_dims\n",
    "    distances = np.sqrt((true_dims-inv_dims)**2).sum(axis=1)\n",
    "    \n",
    "    # Return the computed distances\n",
    "    return distances\n",
    "\n",
    "stimuli_full_path = glob.glob(os.path.join(true_color_stim_dir,'*.png'))\n",
    "stimuli_full_path.sort()\n",
    "\n",
    "# extract the image name from the stimulus path\n",
    "stimuli = [os.path.basename(stim) for stim in stimuli_full_path]\n",
    "\n",
    "my_dict = {\n",
    "    'stim': stimuli,\n",
    "    'min_uv_dist': [],\n",
    "    'max_uv_dist': [],\n",
    "    'mean_uv_dist': [],\n",
    "    'median_uv_dist': [],\n",
    "    'min_ab_dist': [],\n",
    "    'max_ab_dist': [],\n",
    "    'mean_ab_dist': [],\n",
    "    'median_ab_dist': []\n",
    "}\n",
    "\n",
    "for stim in stimuli:\n",
    "    # output directories\n",
    "    true_color_input_dir = os.path.join(true_color_stim_dir, stim)\n",
    "    max_saturation_output_dir = os.path.join(max_saturation_stim_dir, stim)\n",
    "    inverted_hue_output_dir = os.path.join(inverted_hue_stim_dir, stim)\n",
    "    inverted_lab_output_dir = os.path.join(inverted_lab_stim_dir, stim)\n",
    "    inverted_luv_output_dir = os.path.join(inverted_luv_stim_dir, stim)\n",
    "    \n",
    "    # read in the original image\n",
    "    img = cv2.imread(true_color_input_dir)\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Threshold the image to isolate the non-white pixels\n",
    "    _, thresh = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # Find the contours in the thresholded image\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Find the contour with the largest area\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    # Create a binary mask for the largest contour\n",
    "    mask = np.zeros(gray.shape, dtype=np.uint8)\n",
    "    cv2.drawContours(mask, [largest_contour], -1, (255, 255, 255), cv2.FILLED)\n",
    "\n",
    "    # Create a binary mask with the pixels inside the contour set to white\n",
    "    # and the pixels outside the contour set to black\n",
    "    binary_mask = cv2.inRange(mask, 255, 255)\n",
    "    \n",
    "    # convert image from BGR to HLS space\n",
    "    hls_img = cv2.cvtColor(img, cv2.COLOR_BGR2HLS)\n",
    "    # increase saturation \n",
    "    H,L,S = cv2.split(hls_img)\n",
    "    S_high = S + 255 - S.max()\n",
    "    hls_img = cv2.merge((H,L,S_high))\n",
    "    # create image with max saturation values\n",
    "    max_saturation_img = cv2.cvtColor(hls_img,cv2.COLOR_HLS2BGR)\n",
    "    # convert into LAB and LUV space for inversion\n",
    "    lab_img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    luv_img = cv2.cvtColor(img, cv2.COLOR_BGR2LUV)\n",
    "    \n",
    "    # The contour also captures empty space surrounded by object boundaries\n",
    "    # We therefore add a mask that includes all pixels with a luminance value of 255 -> white\n",
    "    white_mask = hls_img[:,:,1] != 255\n",
    "    # black_mask = hls_img[:,:,1]==0\n",
    "    big_mask = np.logical_and(white_mask, binary_mask)\n",
    "    \n",
    "    # save the big mask\n",
    "    big_mask_img = big_mask.astype(np.uint8)*255\n",
    "    big_mask_img = cv2.cvtColor(big_mask_img, cv2.COLOR_GRAY2BGR)\n",
    "    cv2.imwrite(os.path.join(mask_dir,stim), big_mask_img)\n",
    "    \n",
    "    # create a histogram of the hue values in the image for later mask creation\n",
    "    fig = plt.figure()\n",
    "    plt.hist(H[big_mask],bins=50)\n",
    "    plt.savefig(os.path.join(hue_dist_plot_dir,stim))\n",
    "    plt.close(fig=fig)\n",
    "\n",
    "    # create a copy of the hls_img and invert the hue\n",
    "    inverted_hls_img = np.copy(hls_img)\n",
    "    # select all pixels that are not white and increase the hue value by 90 (hue ranges from 0-179)\n",
    "    inverted_hls_img[np.where(binary_mask != 0)]+=np.array([90,0,0],dtype='uint8')\n",
    "    # Split the L*a*b* image into its channels\n",
    "    L, a, b = cv2.split(lab_img)\n",
    "    # Invert the a and b channels\n",
    "    a_inv = 255 - a\n",
    "    b_inv = 255 - b\n",
    "    \n",
    "    # get distances\n",
    "    # ab_distances = calc_dist(a,b,big_mask)\n",
    "    # my_dict['min_ab_dist'].append(ab_distances.min())\n",
    "    # my_dict['max_ab_dist'].append(ab_distances.max())\n",
    "    # my_dict['mean_ab_dist'].append(ab_distances.mean())\n",
    "    # my_dict['median_ab_dist'].append(np.median(ab_distances))\n",
    "    \n",
    "    # plot histogram of distances\n",
    "    fig = plt.figure()\n",
    "    plt.hist(ab_distances,bins=50)\n",
    "    plt.savefig(os.path.join(ab_dist_plot_dir,stim))\n",
    "    plt.close(fig=fig)\n",
    "\n",
    "    # Merge the inverted channels back into the L*a*b* image\n",
    "    inverted_lab_img = cv2.merge((L, a_inv, b_inv))\n",
    "    \n",
    "    # do the same for LUV space\n",
    "    L, u, v = cv2.split(luv_img)\n",
    "    u_inv = 255 - u\n",
    "    v_inv = 255 - v\n",
    "    \n",
    "    # uv_distances = calc_dist(u,v,big_mask)\n",
    "    # my_dict['min_uv_dist'].append(uv_distances.min())\n",
    "    # my_dict['max_uv_dist'].append(uv_distances.max())\n",
    "    # my_dict['mean_uv_dist'].append(uv_distances.mean())\n",
    "    # my_dict['median_uv_dist'].append(np.median(uv_distances))\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.hist(uv_distances,bins=50)\n",
    "    plt.savefig(os.path.join(uv_dist_plot_dir,stim))\n",
    "    plt.close(fig=fig)\n",
    "    \n",
    "    inverted_luv_img = cv2.merge((L, u_inv, v_inv))\n",
    "    \n",
    "    # convert inverted images image back into BGR \n",
    "    inverted_hue_bgr_img = cv2.cvtColor(inverted_hls_img, cv2.COLOR_HLS2BGR)\n",
    "    inverted_lab_bgr_img = cv2.cvtColor(inverted_lab_img, cv2.COLOR_LAB2BGR)\n",
    "    inverted_luv_bgr_img = cv2.cvtColor(inverted_luv_img, cv2.COLOR_LUV2BGR)\n",
    "    \n",
    "    # add alpha channel to inverted image by converting from BGR to BGRA space\n",
    "    max_saturation_bgra_img = cv2.cvtColor(max_saturation_img, cv2.COLOR_BGR2BGRA)\n",
    "    inverted_hue_bgra_img = cv2.cvtColor(inverted_hue_bgr_img, cv2.COLOR_BGR2BGRA)\n",
    "    inverted_lab_bgra_img = cv2.cvtColor(inverted_lab_bgr_img, cv2.COLOR_BGR2BGRA)\n",
    "    inverted_luv_bgra_img = cv2.cvtColor(inverted_luv_bgr_img, cv2.COLOR_BGR2BGRA)\n",
    "    \n",
    "    # make background (everything outside the mask) transparent (and white)\n",
    "    max_saturation_bgra_img[~big_mask] = np.array([255,255,255,0],dtype='uint8')\n",
    "    inverted_hue_bgra_img[~big_mask] = np.array([255,255,255,0],dtype='uint8')\n",
    "    inverted_lab_bgra_img[~big_mask] = np.array([255,255,255,0],dtype='uint8')\n",
    "    inverted_luv_bgra_img[~big_mask] = np.array([255,255,255,0],dtype='uint8')\n",
    "    \n",
    "    # save images\n",
    "    cv2.imwrite(max_saturation_output_dir, max_saturation_bgra_img)\n",
    "    cv2.imwrite(inverted_hue_output_dir, inverted_hue_bgra_img)\n",
    "    cv2.imwrite(inverted_lab_output_dir, inverted_lab_bgra_img)\n",
    "    cv2.imwrite(inverted_luv_output_dir, inverted_luv_bgra_img)\n",
    "    \n",
    "# save information about distances\n",
    "# my_df = pd.DataFrame(data=my_dict,columns=my_dict.keys())\n",
    "# my_df.to_csv(os.path.join(results_dir,'color_distances.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38859f0e-802d-474f-9bce-babd31d3c588",
   "metadata": {},
   "source": [
    "## Masks\n",
    "Create masks that can later be used to _either_ add a hue to a gray-scaled image (two hues can be used gained from the equiluminant flicker method) _or_ as masks to the MATLAB _SHINE_ toolbox<br>\n",
    "We extract everything that is white and everything that lies outside of the \"typical\" color from the object.\n",
    "Gained by selecting (manually) the hue range we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "ba746e8a-74a3-4e27-ab58-9c2d0a7378ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_color_mask(path, lower_threshold=0, upper_threshold=180):\n",
    "    '''This function loads an image and create a mask of all pixels that lie within the object in question, \n",
    "    are not pure white and lie within a hue range.\n",
    "    \n",
    "    Input \n",
    "    path: path to the image that needs to be masked\n",
    "    lower_threshold: lower threshold of the hue\n",
    "    upper_threshold: upper threshold of the hue\n",
    "    \n",
    "    Output: mask\n",
    "    \n",
    "    The function first creates a mask covering the object by drawing a contour around the largest non-white object \n",
    "    (getting rid of unwanted pixels in the periphery).\n",
    "    Since some images contain multiple objects that enclose a white surface the function create a second mask of \n",
    "    ALL white pixels.\n",
    "    \n",
    "    In a last step the function creates a mask of all pixels that lie within a range of hue values.\n",
    "    This way we avoid pixels making up the a structure of the object that has a different memory color\n",
    "    (e.g., yellow seeds in a strawberry)'''\n",
    "    img = cv2.imread(path)\n",
    "    \n",
    "    # read in the original image\n",
    "    img = cv2.imread(path)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Threshold the image to isolate the non-white pixels\n",
    "    _, thresh = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # Find the contours in the thresholded image\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Find the contour with the largest area\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    # Create a binary mask for the largest contour\n",
    "    mask = np.zeros(gray.shape, dtype=np.uint8)\n",
    "    cv2.drawContours(mask, [largest_contour], -1, (255, 255, 255), cv2.FILLED)\n",
    "\n",
    "    # Create a binary mask with the pixels inside the contour set to white\n",
    "    # and the pixels outside the contour set to black\n",
    "    binary_mask = cv2.inRange(mask, 255, 255)\n",
    "\n",
    "    # convert image from BGR to HLS space\n",
    "    hls_img = cv2.cvtColor(img, cv2.COLOR_BGR2HLS)\n",
    "    H,L,S = cv2.split(hls_img)\n",
    "\n",
    "    # The contour also captures empty space surrounded by object boundaries\n",
    "    # We therefore add a mask that includes all pixels with a luminance value of 255 -> white\n",
    "    white_mask = L != 255\n",
    "\n",
    "    # Create a binary mask for pixels within the hue range\n",
    "    hue_mask = cv2.inRange(H, lower_threshold, upper_threshold)\n",
    "    \n",
    "    # combine all masks into one big mask\n",
    "    new_mask = white_mask & binary_mask & hue_mask\n",
    "    \n",
    "    return new_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5542ba8-e55a-4df7-be2a-3767bb28465e",
   "metadata": {},
   "source": [
    "#### Create the masks for the objects\n",
    "Visually inspecting the histogram plots in `../stimuli/analysis/hue_distributions`<br>\n",
    "Select the value deemed suitable and create the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "0094b073-48f4-4cda-adb4-a762723491cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the csv file containing upper and lower limits for the color hue threshold\n",
    "color_threshold_df = pd.read_csv(os.path.join(stim_dir, 'masking_thresholds.csv'))\n",
    "# iterate over dataframe and create new masks based on values\n",
    "for idx, item in color_threshold_df.iterrows():\n",
    "    path = os.path.join(true_color_stim_dir,item.Stimulus)\n",
    "    new_mask = create_color_mask(path=path, lower_threshold=item.lower, upper_threshold=item.upper)\n",
    "    # convert mask from bool to black and white\n",
    "    new_mask = new_mask.astype(np.uint8)*255\n",
    "    # make mask three dimensional (for the shine toolbox)\n",
    "    new_mask = cv2.cvtColor(new_mask, cv2.COLOR_GRAY2BGR)\n",
    "    cv2.imwrite(os.path.join(mask_dir,item.Stimulus), new_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511b967f-23fc-4dac-9cdd-aa43c480510e",
   "metadata": {},
   "source": [
    "### Luminance sanity check\n",
    "Changing the color in LAB space resulted in pixels not representable in RGB space.<br>\n",
    "In turn the conversion back results in different colors with different luminances<br>\n",
    "<br>\n",
    "Check the actual differences of luminance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "id": "dff01246-4bc7-420f-9c97-735daed47e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 16.967773  54.0625   -73.640625]]]\n",
      "[[[ 16.967773 -54.05369   73.64711 ]]]\n",
      "[[[0.         0.21287857 0.        ]]]\n",
      "[[[ 18.64624  -28.78125   25.765625]]]\n"
     ]
    }
   ],
   "source": [
    "green_pixel = np.array([[[0.0,0.0,0.6]]],dtype=np.float32)\n",
    "green_lab = cv2.cvtColor(green_pixel, cv2.COLOR_RGB2LAB)\n",
    "print(green_lab)\n",
    "L, a, b = cv2.split(green_lab)\n",
    "magnitude, angle = cv2.cartToPolar(a, b, angleInDegrees=True)\n",
    "angle_new = angle + 180.0\n",
    "a_new, b_new = cv2.polarToCart(magnitude, angle_new, angleInDegrees=True)\n",
    "green_lab_inv = cv2.merge((L,a_new,b_new))\n",
    "print(green_lab_inv)\n",
    "green_rgb_inv = cv2.cvtColor(green_lab_inv, cv2.COLOR_LAB2RGB)\n",
    "print(green_rgb_inv)\n",
    "sanity = cv2.cvtColor(green_rgb_inv, cv2.COLOR_RGB2LAB)\n",
    "print(sanity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "id": "f55ac0de-aaf2-4966-9e7f-a1a1c93afb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[123 200 184]]]\n",
      "[[[123  55  71]]]\n",
      "[[[146 120  85]]]\n"
     ]
    }
   ],
   "source": [
    "red_pixel = np.array([[[227,  19,  18]]], np.uint8)\n",
    "red_lab = cv2.cvtColor(red_pixel, cv2.COLOR_RGB2LAB)\n",
    "print(red_lab)\n",
    "L_red, a_red, b_red = cv2.split(red_lab)\n",
    "a_inv = 255-a_red\n",
    "b_inv = 255-b_red\n",
    "inv_lab = cv2.merge((L_red, a_inv, b_inv))\n",
    "print(inv_lab)\n",
    "inv_rgb = cv2.cvtColor(inv_lab, cv2.COLOR_LAB2RGB)\n",
    "inv_lab_sanity = cv2.cvtColor(inv_rgb, cv2.COLOR_RGB2LAB)\n",
    "print(inv_lab_sanity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe0899-49b0-42a6-b1ce-aa85a90d17e5",
   "metadata": {},
   "source": [
    "## Best Luminance\n",
    "Load an image and try different luminance settings to find one with the lowest number of pixels with changed Luminacne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4c357708-6e9b-4831-8fa4-f2735ac11843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131.07759475708008\n",
      "204.80415725708008\n",
      "141.58540725708008\n",
      "172.80415725708008\n",
      "204.80415725708008\n",
      "80.70259475708008\n",
      "65.30708694458008\n",
      "204.80415725708008\n",
      "188.80415725708008\n",
      "196.80415725708008\n",
      "204.80415725708008\n",
      "57.40571975708008\n",
      "65.30708694458008\n",
      "61.39400100708008\n",
      "57.40571975708008\n",
      "53.41450881958008\n",
      "55.41157913208008\n",
      "57.40571975708008\n",
      "59.40181350708008\n",
      "58.40474319458008\n",
      "57.40571975708008\n",
      "56.41157913208008\n",
      "56.41157913208008\n",
      "No change in lum diff.\n",
      "Percentage of pixels with changed luminance:\n",
      "0.9857373230603834\n",
      "56.41157913208008\n",
      "No change in lum diff.\n",
      "Percentage of pixels with changed luminance:\n",
      "0.9857373230603834\n",
      "56.41157913208008\n",
      "No change in lum diff.\n",
      "Percentage of pixels with changed luminance:\n",
      "0.9857373230603834\n"
     ]
    }
   ],
   "source": [
    "def calc_lum_diff(L,a,b, mask):\n",
    "    new_a = 255-a\n",
    "    new_b = 255-b\n",
    "    new_img = cv2.merge((L, new_a, new_b))\n",
    "    new_rgb = cv2.cvtColor(new_img, cv2.COLOR_LAB2RGB)\n",
    "    new_lab = cv2.cvtColor(new_rgb, cv2.COLOR_RGB2LAB)\n",
    "    new_L, a, b = cv2.split(new_lab)\n",
    "    L_1_f = L.astype(np.float32)\n",
    "    L_2_f = new_L.astype(np.float32)\n",
    "    lum_diff = (L_1_f-L_2_f)[mask==1]\n",
    "    return sum(lum_diff!=0)/len(lum_diff)\n",
    "\n",
    "stim = 'red_strawberry.png'\n",
    "\n",
    "true_path = os.path.join(true_color_stim_dir, stim)\n",
    "mask_path = os.path.join(mask_dir, stim)\n",
    "mask_img = cv2.imread(mask_path)\n",
    "mask_img = mask_img[:,:,0].astype(bool)\n",
    "\n",
    "true_color_img = cv2.imread(true_path)\n",
    "old_lab = cv2.cvtColor(true_color_img, cv2.COLOR_BGR2LAB)\n",
    "old_L, a, b = cv2.split(old_lab)\n",
    "old_lum_diff = calc_lum_diff (old_L, a, b, mask_img)\n",
    "change = 128\n",
    "new_L = old_L + change\n",
    "new_L = new_L.astype(np.uint8)\n",
    "\n",
    "for i in range(25):\n",
    "    print(new_L.mean(axis=None))\n",
    "    new_lum_diff = calc_lum_diff (new_L, a, b, mask_img)\n",
    "    if old_lum_diff>new_lum_diff:\n",
    "        right_direction = True\n",
    "        new_L = new_L + change\n",
    "    elif old_lum_diff<new_lum_diff:\n",
    "        right_direction = False\n",
    "        change *= -0.5\n",
    "        new_L = new_L + change\n",
    "    elif old_lum_diff==new_lum_diff:\n",
    "        print('No change in lum diff.')\n",
    "        print('Percentage of pixels with changed luminance:')\n",
    "        print(new_lum_diff)\n",
    "        \n",
    "    new_L = new_L.astype(np.uint8)\n",
    "    old_lum_diff = new_lum_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d5999be6-2921-4ddf-bce3-ea7e4554e82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([159122.,      0.,   2389.,   9441.,  35537.,  37968.,   8827.,\n",
       "          3433.,   4623.,    804.]),\n",
       " array([  1. ,  26.4,  51.8,  77.2, 102.6, 128. , 153.4, 178.8, 204.2,\n",
       "        229.6, 255. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(new_L.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "29751c7a-ce0a-4b27-adac-9616c6bf0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lum_diff_dir = os.path.join(results_dir, 'lum_diff')\n",
    "if not os.path.exists(lum_diff_dir):\n",
    "    os.mkdir(lum_diff_dir)\n",
    "    \n",
    "lum_diff_sum = []\n",
    "lum_diff_percent = []\n",
    "\n",
    "for stim in stimuli:\n",
    "    true_path = os.path.join(true_color_stim_dir, stim)\n",
    "    inv_path = os.path.join(inverted_lab_stim_dir, stim)\n",
    "    mask_path = os.path.join(mask_dir, stim)\n",
    "    mask_img = cv2.imread(mask_path)\n",
    "    mask_img = mask_img[:,:,0].astype(bool)\n",
    "\n",
    "    true_color_img = cv2.imread(true_path)\n",
    "    true_lab = cv2.cvtColor(true_color_img, cv2.COLOR_BGR2LAB)\n",
    "    true_L, a, b = cv2.split(true_lab)\n",
    "    \n",
    "    inv_color_img = cv2.imread(inv_path)\n",
    "    inv_lab = cv2.cvtColor(inv_color_img, cv2.COLOR_BGR2LAB)\n",
    "    inv_L, a, b = cv2.split(inv_lab)\n",
    "\n",
    "    true_L_f = true_L.astype(np.float32)\n",
    "    inv_L_f = inv_L.astype(np.float32)\n",
    "    \n",
    "    lum_diff = (true_L_f-inv_L_f)[mask_img==1]\n",
    "    lum_diff_sum.append(lum_diff.sum())\n",
    "    lum_diff_percent.append(sum(lum_diff!=0)/len(lum_diff))\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.hist(lum_diff, bins=40)\n",
    "    plt.savefig(os.path.join(lum_diff_dir,stim))\n",
    "    plt.close(fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aa90df15-e05f-4d5f-b64f-52313ce64fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.03735211738639689,\n",
       " 0.015230427655468637,\n",
       " 0.029940220736406332,\n",
       " 0.012770170605707513,\n",
       " 0.02350679046563193,\n",
       " 0.04111380249083621,\n",
       " 0.07935067984375334,\n",
       " 0.06503392354790667,\n",
       " 0.10305563295254017,\n",
       " 0.21234412218018775,\n",
       " 0.983418069872662,\n",
       " 0.9889035038620844,\n",
       " 0.9921902050552072,\n",
       " 0.9628404049317352,\n",
       " 0.9955213033852186,\n",
       " 0.8988896836998103,\n",
       " 0.9924381815733968,\n",
       " 0.9548019479443887,\n",
       " 0.8979415918909937,\n",
       " 0.8836485379039585,\n",
       " 0.8473376108796237,\n",
       " 0.9002146221813419,\n",
       " 0.6014127409274723,\n",
       " 0.5808891087709379,\n",
       " 0.9758709710400532,\n",
       " 0.9769521976259223,\n",
       " 0.9801195963978364,\n",
       " 0.985892611837783,\n",
       " 0.9454443979349798,\n",
       " 0.9618718391908329,\n",
       " 0.9739328365189736,\n",
       " 0.9611659356725146,\n",
       " 0.9817083901623477,\n",
       " 0.9900623279329437,\n",
       " 0.965714445908441,\n",
       " 0.9593431093548913,\n",
       " 0.9819961342724975,\n",
       " 0.9992424638211169,\n",
       " 0.3693952469093768,\n",
       " 0.394409014553509]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lum_diff_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133eb126-022f-400e-8f72-d50d73a73bba",
   "metadata": {},
   "source": [
    "## Create copies of the original images with compressed RGB values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "id": "289e7553-4284-4630-b85f-a48d254ec1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through stimuli\n",
    "for stim in stimuli:\n",
    "    # read in the original image\n",
    "    true_stim_path = os.path.join(true_color_stim_dir,stim)\n",
    "    true_img = cv2.imread(true_stim_path)\n",
    "    \n",
    "    # compress the RGB values by center them around zero, multiply with a number <1 and restore the center\n",
    "    true_img_f = true_img.astype(np.float32)/255.0\n",
    "    true_img_f -= 0.5\n",
    "    true_img_f *= .7\n",
    "    true_img_f += 0.5\n",
    "    true_img_f *=255.0\n",
    "    \n",
    "    # save image\n",
    "    compressed_img_path = os.path.join(compressed_true_color, stim)\n",
    "    cv2.imwrite(compressed_img_path, true_img_f)\n",
    "    \n",
    "    true_img_f /=255.0\n",
    "    # invert image in LAB space\n",
    "    false_img_f = cv2.cvtColor(true_img_f, cv2.COLOR_BGR2Lab)\n",
    "    L,a,b = cv2.split(false_img_f)\n",
    "    \n",
    "    # Compute the magnitude and angle of the a-b vectors\n",
    "    magnitude, angle = cv2.cartToPolar(a, b, angleInDegrees=True)\n",
    "    angle_new = angle + 180.0\n",
    "    a_new, b_new = cv2.polarToCart(magnitude, angle_new, angleInDegrees=True)\n",
    "    \n",
    "    false_img_f = cv2.merge((L, a_new, b_new))\n",
    "    false_img_bgr = cv2.cvtColor(false_img_f, cv2.COLOR_LAB2BGR)*255.0\n",
    "    \n",
    "    # save image\n",
    "    compressed_false_img_path = os.path.join(compressed_false_color, stim)\n",
    "    cv2.imwrite(compressed_false_img_path, false_img_bgr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d603758-d701-4207-93d6-33e01370d38c",
   "metadata": {},
   "source": [
    "# Example color values for flicker method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "24644bc4-4a9b-4228-bf74-4164543163d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color image\n",
    "stim = 'red_strawberry.png' \n",
    "true_path = os.path.join(true_color_stim_dir,stim)\n",
    "inv_path = os.path.join(inverted_lab_stim_dir,stim)\n",
    "true_color_img = cv2.imread(true_path)\n",
    "true_color_img = cv2.cvtColor(true_color_img, cv2.COLOR_BGR2RGB)\n",
    "inv_color_img = cv2.imread(inv_path)\n",
    "inv_color_img = cv2.cvtColor(inv_color_img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "49d86723-2b90-48a1-8c43-4d8af46de7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_path = os.path.join(mask_dir, stim)\n",
    "new_mask = cv2.imread(mask_path)\n",
    "new_mask = new_mask[:,:,0].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "319f7f99-ee45-4deb-bf85-ac6325c1544f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89040261 0.07737457 0.07445575]\n",
      "[0.00866076 0.57562243 0.83034924]\n"
     ]
    }
   ],
   "source": [
    "true_means = true_color_img[new_mask==1].mean(axis=0)/255\n",
    "inv_means = inv_color_img[new_mask==1].mean(axis=0)/255\n",
    "\n",
    "print(true_means)\n",
    "print(inv_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "531a1658-cc74-465c-82fd-d54bace9e84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color image\n",
    "stim = 'green_brokkoli_1.png' \n",
    "true_path = os.path.join(true_color_stim_dir,stim)\n",
    "inv_path = os.path.join(inverted_lab_stim_dir,stim)\n",
    "true_color_img = cv2.imread(true_path)\n",
    "true_color_img = cv2.cvtColor(true_color_img, cv2.COLOR_BGR2RGB)\n",
    "inv_color_img = cv2.imread(inv_path)\n",
    "inv_color_img = cv2.cvtColor(inv_color_img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "1b8de7ff-554c-4786-813d-11f0f62c456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_path = os.path.join(mask_dir, stim)\n",
    "new_mask = cv2.imread(mask_path)\n",
    "new_mask = new_mask[:,:,0].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec337fc-ff15-4c1f-9d51-572105289ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33625086 0.51528191 0.28597812]\n",
      "[0.5571594  0.41451602 0.65644618]\n"
     ]
    }
   ],
   "source": [
    "true_means = true_color_img[new_mask==1].mean(axis=0)/255\n",
    "inv_means = inv_color_img[new_mask==1].mean(axis=0)/255\n",
    "\n",
    "print(true_means)\n",
    "print(inv_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "bc4a9e21-7928-490f-a4e2-dcf1da9be036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color image\n",
    "stim = 'yellow_banana.png' \n",
    "true_path = os.path.join(true_color_stim_dir,stim)\n",
    "inv_path = os.path.join(inverted_lab_stim_dir,stim)\n",
    "true_color_img = cv2.imread(true_path)\n",
    "true_color_img = cv2.cvtColor(true_color_img, cv2.COLOR_BGR2RGB)\n",
    "inv_color_img = cv2.imread(inv_path)\n",
    "inv_color_img = cv2.cvtColor(inv_color_img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "6eb0a714-bf9a-48dc-852f-fc5191276bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_path = os.path.join(mask_dir, stim)\n",
    "new_mask = cv2.imread(mask_path)\n",
    "new_mask = new_mask[:,:,0].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "26904e84-fdc5-4a71-a6ab-0e8d7534f1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90866778 0.78573948 0.11242274]\n",
      "[0.12867932 0.81841307 0.98263644]\n"
     ]
    }
   ],
   "source": [
    "true_means = true_color_img[new_mask==1].mean(axis=0)/255\n",
    "inv_means = inv_color_img[new_mask==1].mean(axis=0)/255\n",
    "\n",
    "print(true_means)\n",
    "print(inv_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "4108467f-2afe-4b6f-b2df-6fab14db0096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color image\n",
    "stim = 'orange_pumpkin.png' \n",
    "true_path = os.path.join(true_color_stim_dir,stim)\n",
    "inv_path = os.path.join(inverted_lab_stim_dir,stim)\n",
    "true_color_img = cv2.imread(true_path)\n",
    "true_color_img = cv2.cvtColor(true_color_img, cv2.COLOR_BGR2RGB)\n",
    "inv_color_img = cv2.imread(inv_path)\n",
    "inv_color_img = cv2.cvtColor(inv_color_img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "ca1e659d-e765-49fb-8845-ef8139be3563",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_path = os.path.join(mask_dir, stim)\n",
    "new_mask = cv2.imread(mask_path)\n",
    "new_mask = new_mask[:,:,0].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "77cf57a7-c6a7-4cbb-811c-cc11078b1855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90565069 0.39444017 0.11520369]\n",
      "[0.05491524 0.65910068 0.9429003 ]\n"
     ]
    }
   ],
   "source": [
    "true_means = true_color_img[new_mask==1].mean(axis=0)/255\n",
    "inv_means = inv_color_img[new_mask==1].mean(axis=0)/255\n",
    "\n",
    "print(true_means)\n",
    "print(inv_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0189cea7-efa8-4ba7-83b3-68db44982b21",
   "metadata": {},
   "source": [
    "#### an image of the colors for each hue value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "e839c0b6-22e5-4369-ac64-a984c6135714",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_colors = np.zeros((100,180,3),dtype='uint8')\n",
    "all_colors[:,:,0] = np.linspace(0,180,180)\n",
    "all_colors[:,:,1] = 100\n",
    "all_colors[:,:,2] = 150\n",
    "\n",
    "all_colors_rgb = cv2.cvtColor(all_colors, cv2.COLOR_HLS2RGB)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(all_colors_rgb)\n",
    "plt.savefig(os.path.join(mask_dir,'hue_distribution.png'))\n",
    "plt.close(fig=fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
