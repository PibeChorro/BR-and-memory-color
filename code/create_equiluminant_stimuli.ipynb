{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the os\n",
    "import os\n",
    "import glob\n",
    "# math and data structure\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# image processing\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = 'sub-00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_dir = os.path.join('..','stimuli')\n",
    "data_dir = os.path.join('..','rawdata')\n",
    "\n",
    "# taken from original - added alpha channel\n",
    "true_color_stim_dir = os.path.join(stim_dir,'true_color')\n",
    "# stimuli in LAB space inverted\n",
    "inverted_lab_stim_dir = os.path.join(stim_dir,'inverted_lab')\n",
    "# mask to apply the change\n",
    "mask_dir = os.path.join(stim_dir,'color_masks')\n",
    "# forground_background masks to apply the alpha channel\n",
    "fb_mask_dir = os.path.join(stim_dir, 'foreground_background_masks')\n",
    "\n",
    "# subject directory\n",
    "sub_dir = os.path.join(data_dir,sub)\n",
    "# stimulus folder for subject to store subject specific equiluminant images\n",
    "sub_stim_dir = os.path.join(sub_dir,'stimuli')\n",
    "if not os.path.exists(sub_stim_dir):\n",
    "    os.mkdir(sub_stim_dir)\n",
    "sub_true_stim_dir = os.path.join(sub_stim_dir, 'true_color')\n",
    "if not os.path.exists(sub_true_stim_dir):\n",
    "    os.mkdir(sub_true_stim_dir)\n",
    "sub_inv_stim_dir = os.path.join(sub_stim_dir, 'inverted_color')\n",
    "if not os.path.exists(sub_inv_stim_dir):\n",
    "    os.mkdir(sub_inv_stim_dir)\n",
    "\n",
    "# directory where to store images where pixels were clipped\n",
    "clipped_stimuli_dir = os.path.join(sub_dir,'clipped_stimuli')\n",
    "if not os.path.exists(clipped_stimuli_dir):\n",
    "    os.mkdir(clipped_stimuli_dir)\n",
    "true_clipped_dir = os.path.join(clipped_stimuli_dir,'true_color')\n",
    "if not os.path.exists(true_clipped_dir):\n",
    "    os.mkdir(true_clipped_dir)\n",
    "inv_clipped_dir = os.path.join(clipped_stimuli_dir,'inverted_color')\n",
    "if not os.path.exists(inv_clipped_dir):\n",
    "    os.mkdir(inv_clipped_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all stimuli in the directory\n",
    "stimuli_full_path = glob.glob(os.path.join(true_color_stim_dir,'*.png'))\n",
    "stimuli_full_path.sort()\n",
    "\n",
    "# extract the image name from the stimulus path\n",
    "stimuli = [os.path.basename(stim) for stim in stimuli_full_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the table containing the typical color values for all \n",
    "color_table = pd.read_csv(os.path.join(stim_dir,'representative_pixels.csv'))\n",
    "# read in the subject specific luminance correction table\n",
    "equilum_table = pd.read_csv(os.path.join(sub_dir,'equiluminantColorTable.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over entries in the equiluminance table\n",
    "# read in the original (true and inverted color) stimuli, \n",
    "# normalize it to the \"typical pixel\" and \n",
    "# multiply with the luminance correction factor\n",
    "for (_,color_item), (_, lum_item) in zip(color_table.iterrows(), equilum_table.iterrows()): \n",
    "    # get stimuli paths\n",
    "    cur_true_path = os.path.join(true_color_stim_dir, lum_item.stimuli)\n",
    "    cur_inv_path = os.path.join(inverted_lab_stim_dir, lum_item.stimuli)\n",
    "    # get mask paths\n",
    "    cur_mask_path = os.path.join(mask_dir, lum_item.stimuli)\n",
    "    cur_fb_mask_path = os.path.join(fb_mask_dir, lum_item.stimuli)\n",
    "    \n",
    "    cur_true_img = cv2.imread(cur_true_path)\n",
    "    cur_inv_img = cv2.imread(cur_inv_path)\n",
    "    cur_mask_img = cv2.imread(cur_mask_path)\n",
    "    cur_fb_mask_img = cv2.imread(cur_fb_mask_path)\n",
    "    # convert the images into float so that you can change them better and they are not ceiled\n",
    "    cur_true_img = cur_true_img.astype(np.float32)\n",
    "    cur_inv_img = cur_inv_img.astype(np.float32)\n",
    "    # convert BGR images to RGBA for clarity and to later make \n",
    "    cur_true_img = cv2.cvtColor(cur_true_img, cv2.COLOR_BGR2RGBA)\n",
    "    cur_inv_img = cv2.cvtColor(cur_inv_img, cv2.COLOR_BGR2RGBA)\n",
    "    # masks also have three channels with either 0 or 255 - make it a two dimensional bool mask\n",
    "    cur_mask_img = cur_mask_img[:,:,0].astype(bool)\n",
    "    cur_fb_mask_img = cur_fb_mask_img[:,:,0].astype(bool)\n",
    "    # get channels of images\n",
    "    true_r, true_g, true_b, true_a = cv2.split(cur_true_img)\n",
    "    inv_r, inv_g, inv_b, inv_a = cv2.split(cur_inv_img)\n",
    "    \n",
    "    # devide the color channels by their typical color value and multiply with the equiluminant factor\n",
    "    true_r[cur_mask_img] = true_r[cur_mask_img]/color_item.true_R * lum_item.true_R * 255\n",
    "    true_g[cur_mask_img] = true_g[cur_mask_img]/color_item.true_G * lum_item.true_G * 255\n",
    "    true_b[cur_mask_img] = true_b[cur_mask_img]/color_item.true_B * lum_item.true_B * 255\n",
    "    \n",
    "    inv_r[cur_mask_img] = inv_r[cur_mask_img]/color_item.inv_R * lum_item.inv_R * 255\n",
    "    inv_g[cur_mask_img] = inv_g[cur_mask_img]/color_item.inv_G * lum_item.inv_G * 255\n",
    "    inv_b[cur_mask_img] = inv_b[cur_mask_img]/color_item.inv_B * lum_item.inv_B * 255\n",
    "    \n",
    "    # add alpha channel\n",
    "    true_a[cur_fb_mask_img] = 255\n",
    "    inv_a[cur_fb_mask_img] = 255\n",
    "    \n",
    "    # combine the channels to the corrected images (CAUTION! the correct order for opencv is BGR)\n",
    "    corr_true_img = cv2.merge((true_b, true_g, true_r, true_a))\n",
    "    corr_inv_img = cv2.merge((inv_b, inv_g, inv_r, inv_a))\n",
    "    \n",
    "    # get all pixels that are larger than 255 and save them to know how many and which pixels were capped\n",
    "    capped_true_img = corr_true_img.copy()\n",
    "    capped_true_img -= 255\n",
    "    capped_true_img[capped_true_img<0] = 0\n",
    "    \n",
    "    capped_inv_img = corr_inv_img.copy()\n",
    "    capped_inv_img -= 255\n",
    "    capped_inv_img[capped_inv_img<0] = 0\n",
    "    \n",
    "    capped_true_img = capped_true_img.astype(np.uint8)\n",
    "    capped_inv_img = capped_inv_img.astype(np.uint8)\n",
    "    \n",
    "    cv2.imwrite(os.path.join(true_clipped_dir,lum_item.stimuli), capped_true_img)\n",
    "    cv2.imwrite(os.path.join(inv_clipped_dir,lum_item.stimuli), capped_inv_img)\n",
    "    # ceil the images that they are not larger than 255\n",
    "    corr_true_img[corr_true_img>255] = 255\n",
    "    corr_inv_img[corr_inv_img>255] = 255\n",
    "    \n",
    "    # convert back into unsigned int\n",
    "    corr_true_img = corr_true_img.astype(np.uint8)#(np.float32)/255.0\n",
    "    corr_inv_img = corr_inv_img.astype(np.uint8)#(np.float32)/255.0\n",
    "    \n",
    "    # write image into subject stimuli folder\n",
    "    cv2.imwrite(os.path.join(sub_true_stim_dir,lum_item.stimuli),corr_true_img)\n",
    "    cv2.imwrite(os.path.join(sub_inv_stim_dir,lum_item.stimuli),corr_inv_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
